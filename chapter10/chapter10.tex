\documentclass{article}
% for general math stuff
\usepackage{amsmath}
% for proof
\usepackage{amsthm}
% for number set symbols
\usepackage{amsfonts}
% for links
\usepackage{hyperref}

\author{Arthur Allshire}
\title{A Programmer's Introduction to Mathematics: Chapter 10 Exercise solutions}

\theoremstyle{remark}
\newtheorem*{exercise}{Exercise}
\newtheorem*{lemma}{Lemma}

\newcount\colveccount
\newcommand*\colvec[1]{
	\global\colveccount#1
	\begin{bmatrix}
		\colvecnext
	}
	\def\colvecnext#1{
		#1
		\global\advance\colveccount-1
		\ifnum\colveccount>0
		\\
		\expandafter\colvecnext
		\else
	\end{bmatrix}
	\fi
}

\begin{document}
\maketitle

\section*{Theorem 10.14 Proof}
\begin{exercise}
    Let $U, V, W$ be three vector spaces. Let $f: U \rightarrow V$ and $g: V \rightarrow W$ be linear maps. Then
\[
    M(g \circ f) = M(g)M(f)
\]
where $g \circ f$ denotes the function composition $x \mapsto g(f(x))$, and $M(g)M(f)$ denotes matrix multiplication.
\end{exercise}
Let $n,m,l \in \mathbb{N}$ be the respective dimensions of $U, V, W$, and
$x \in U$ be an arbitrary vector. First, a lemma to isolate all of the messy index stuff.

\begin{lemma}
Where $x \in U$, we have $M(g)(M(f)x) = (M(g)M(f))x$.
\end{lemma}

\begin{proof}
Let
\[
\begin{aligned}
    x &= \colvec{3}{\alpha_1}{\vdots}{\alpha_n} \text{, }
    M(f) =
    \begin{bmatrix}
        \beta_{1,1} & \dots & \beta_{1,n} \\
        \vdots & \ddots & \vdots \\
        \beta_{m,1} & \dots & \beta_{m,n}
    \end{bmatrix} \text{, and }
    M(g) =
    \begin{bmatrix}
        \delta_{1,1} & \dots & \delta_{1,m} \\
        \vdots & \ddots & \vdots \\
        \delta_{l,1} & \dots & \delta_{l,m}
    \end{bmatrix} \text{. So, } \\
    M(f)x &=
    \colvec{3}
    {\sum_{i=1}^{n} \alpha_i \beta_{i, 1}}
    {\vdots}
    {\sum_{i=1}^{n} \alpha_i \beta_{i, m}} \\
    M(g)(M(f)x) &=
    \begin{bmatrix}
        \delta_{1,1} & \dots & \delta_{1,m} \\
        \vdots & \ddots & \vdots \\
        \delta_{l,1} & \dots & \delta_{l,m}
    \end{bmatrix}
    \colvec{3}
    {\sum_{i=1}^{n} \alpha_i \beta_{i, 1}}
    {\vdots}
    {\sum_{i=1}^{n} \alpha_i \beta_{i, m}}
    = \colvec{3}
    {\sum_{j=1}^{m} \delta_{1, j} \sum_{i=1}^{n} \alpha_i \beta_{i, j}}
    {\vdots}
    {\sum_{j=1}^{m} \delta_{l, j} \sum_{i=1}^{n} \alpha_i \beta_{i, j}} \\
    &= \colvec{3}
    {\sum_{j=1}^{m} \sum_{i=1}^{n} \delta_{1, j}  \alpha_i \beta_{i, j}}
    {\vdots}
    {\sum_{j=1}^{m} \sum_{i=1}^{n} \delta_{l, j} \alpha_i \beta_{i, j}} \text{. Also, } \\
    M(g)M(f) &=
    \begin{bmatrix}
        \delta_{1,1} & \dots & \delta_{1,m} \\
        \vdots & \ddots & \vdots \\
        \delta_{l,1} & \dots & \delta_{l,m}
    \end{bmatrix}
    \begin{bmatrix}
        \beta_{1,1} & \dots & \beta_{1,n} \\
        \vdots & \ddots & \vdots \\
        \beta_{m,1} & \dots & \beta_{m,n}
    \end{bmatrix}
    =
    \begin{bmatrix}
        \sum_{j=1}^{m} \delta_{1, j} \beta_{j, 1} & \dots & \sum_{j=1}^{m} \delta_{1, j} \beta_{j, n} \\
        \vdots & \ddots & \vdots \\
        \sum_{j=1}^{m} \delta_{l, j} \beta_{j, 1} & \dots & \sum_{j=1}^{m} \delta_{l, j} \beta_{j, n} \\
    \end{bmatrix} \\
    (M(g)M(f))x &=
    \begin{bmatrix}
        \sum_{j=1}^{m} \delta_{1, j} \beta_{j, 1} & \dots & \sum_{j=1}^{m} \delta_{1, j} \beta_{j, n} \\
        \vdots & \ddots & \vdots \\
        \sum_{j=1}^{m} \delta_{l, j} \beta_{j, 1} & \dots & \sum_{j=1}^{m} \delta_{l, j} \beta_{j, n} \\
    \end{bmatrix}
    \colvec{3}{\alpha_1}{\vdots}{\alpha_n}
    \\ &= \colvec{3}
    { \sum_{i=1}^n \alpha_i \sum_{j=1}^{m}  \delta_{1,j} \beta_{j,i} }
    {\vdots}
    { \sum_{i=1}^n \alpha_i \sum_{j=1}^{m}  \delta_{l,j} \beta_{j,i} }
    = \colvec{3}
    {\sum_{j=1}^{m} \sum_{i=1}^{n} \delta_{1, j}  \alpha_i \beta_{i, j}}
    {\vdots}
    {\sum_{j=1}^{m} \sum_{i=1}^{n} \delta_{l, j} \alpha_i \beta_{i, j}} \text{(order doesn't matter)}\\
    &= M(g)(M(f)x) \text{, as required.}
\end{aligned}
\]
\end{proof}

Using this, we may fairly simply complete the proof.

\begin{proof}
By definition, $g(f(x)) = M(g \circ f)x$. We also know that $f(x) = M(f)x$ Thus, $g(f(x)) = g(M(f)x) = M(g)(M(f)x) = (M(g)M(f))x$ (using the lemma)
and hence the function composition in matrix form $M(g \circ f) = M(g)M(f)$.
\end{proof}

\section*{Map to matrix preserves inverses}

\begin{exercise}
	From page 156: Prove that if a linear map is a bijection, then its inverse is also a linear map, and the linear-map-to-matrix correspondence preserves inverses.
\end{exercise}
First we prove the linearity of the inverse
\begin{proof}
	Let $V, W$ be two vector spaces, and $f: V \mapsto W$ be a linear bijective map.
	Since $f$ is a bijection, there exists an inverse $f^{-1}: W \mapsto V$.
	Let $x, y \in V$, and $f(x) = x', \ f(y) = y'$. Let $c$ be a scalar.
	Now,

\[
\begin{aligned}
f(x+y) &= f(x) + f(y) \text{, by linearity of f, } \\
x + y &= f^{-1}(f(x) + f(y)) \text{, taking the inverse, } \\
f^{-1}(x') + f^{-1}(y') &= f^{-1}(x' + y') \\
f(cx) &= cf(x) \text{, by linearity of f} \\
cx &= f^{-1}(cf(x)) \text{, taking the inverse, and hence } \\
cf^{-1}(x') &= f^{-1}(cx') \\
\end{aligned}
\]
And hence $f^{-1}$ is linear, as required.
\end{proof}

\end{document}
